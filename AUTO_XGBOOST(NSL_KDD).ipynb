{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNqvXlnuNrcjLkE8qcJLuWg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/allenjose24/BankApp/blob/main/AUTO_XGBOOST(NSL_KDD).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0y0x5zmJEo1",
        "outputId": "d5adb68c-c8e2-459f-e98b-fef86172da5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting script...\n",
            "Successfully loaded KDDTrain+.txt and KDDTest+.txt\n",
            "Starting pre-processing...\n",
            "Data pre-processed. Final feature count: 122\n",
            "\n",
            "--- Training Model 1: XGBoost (Supervised) ---\n",
            "XGBoost training complete.\n",
            "\n",
            "--- Training Model 2: Autoencoder (Unsupervised) ---\n",
            "Autoencoder will be trained on 67343 'normal' samples.\n",
            "Autoencoder training complete.\n",
            "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "\u001b[1m2105/2105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
            "Anomaly threshold (95th percentile of normal error) set to: 0.000277\n",
            "\n",
            "==================================================\n",
            "           MODEL PERFORMANCE COMPARISON\n",
            "==================================================\n",
            "\n",
            "--- Metrics for XGBoost (Supervised) ---\n",
            "Accuracy:    0.7879\n",
            "Precision:   0.9681\n",
            "Recall:      0.6487\n",
            "F1-Score:    0.7769\n",
            "AUC-ROC:     0.8103\n",
            "\n",
            "Confusion Matrix:\n",
            "[[9437  274]\n",
            " [4508 8325]]\n",
            "\n",
            "\n",
            "--- Metrics for Autoencoder (Unsupervised) ---\n",
            "Accuracy:    0.8799\n",
            "Precision:   0.9151\n",
            "Recall:      0.8697\n",
            "F1-Score:    0.8918\n",
            "AUC-ROC:     0.8815\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 8675  1036]\n",
            " [ 1672 11161]]\n",
            "\n",
            "\n",
            "==================================================\n",
            "Script finished.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import load_model\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Starting script...\")\n",
        "\n",
        "# === 1. LOAD DATA & ADD COLUMN HEADERS ===\n",
        "\n",
        "# These are the 41 feature names + 2 label columns for NSL-KDD\n",
        "col_names = [\n",
        "    'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot',\n",
        "    'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations',\n",
        "    'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count', 'srv_count',\n",
        "    'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate',\n",
        "    'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n",
        "    'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate',\n",
        "    'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'label', 'difficulty'\n",
        "]\n",
        "\n",
        "# --- Load Training and Test Data ---\n",
        "# !! Update these paths to where you downloaded the files !!\n",
        "train_path = '/content/KDDTrain+.txt'\n",
        "test_path = '/content/KDDTest+.txt'\n",
        "\n",
        "try:\n",
        "    df_train = pd.read_csv(train_path, header=None, names=col_names)\n",
        "    df_test = pd.read_csv(test_path, header=None, names=col_names)\n",
        "    print(\"Successfully loaded KDDTrain+.txt and KDDTest+.txt\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Could not find dataset files.\")\n",
        "    print(f\"Please make sure '{train_path}' and '{test_path}' are in the same folder as this script.\")\n",
        "    exit()\n",
        "\n",
        "# === 2. PRE-PROCESSING ===\n",
        "\n",
        "print(\"Starting pre-processing...\")\n",
        "\n",
        "# --- 2a. Handle Target Labels ---\n",
        "# We will do a binary classification: 'normal' (0) vs. 'attack' (1)\n",
        "df_train['label'] = (df_train['label'] != 'normal').astype(int)\n",
        "df_test['label'] = (df_test['label'] != 'normal').astype(int)\n",
        "\n",
        "# Drop the 'difficulty' column as it's not needed\n",
        "df_train = df_train.drop('difficulty', axis=1)\n",
        "df_test = df_test.drop('difficulty', axis=1)\n",
        "\n",
        "# --- 2b. Handle Categorical Features (One-Hot Encoding) ---\n",
        "categorical_cols = ['protocol_type', 'service', 'flag']\n",
        "\n",
        "# Combine train and test for consistent one-hot encoding\n",
        "combined_df = pd.concat([df_train.drop('label', axis=1), df_test.drop('label', axis=1)], axis=0)\n",
        "combined_df = pd.get_dummies(combined_df, columns=categorical_cols, dtype=int)\n",
        "\n",
        "# Separate back into train and test\n",
        "X_train = combined_df.iloc[:len(df_train)]\n",
        "X_test = combined_df.iloc[len(df_train):]\n",
        "\n",
        "# Get the target labels\n",
        "y_train = df_train['label']\n",
        "y_test = df_test['label']\n",
        "\n",
        "print(f\"Data pre-processed. Final feature count: {X_train.shape[1]}\")\n",
        "\n",
        "# --- 2c. Feature Scaling ---\n",
        "# This is CRITICAL for Autoencoders and good practice for XGBoost\n",
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# === 3. MODEL 1: XGBOOST (SUPERVISED CLASSIFIER) ===\n",
        "\n",
        "print(\"\\n--- Training Model 1: XGBoost (Supervised) ---\")\n",
        "# Use 'use_label_encoder=False' and 'eval_metric='logloss'' as best practice\n",
        "model_xgb = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "\n",
        "# Train the model on ALL training data (normal + attacks)\n",
        "model_xgb.fit(X_train, y_train)\n",
        "\n",
        "# Get predictions on the test set\n",
        "y_pred_xgb = model_xgb.predict(X_test)\n",
        "\n",
        "print(\"XGBoost training complete.\")\n",
        "\n",
        "# === 4. MODEL 2: AUTOENCODER (UNSUPERVISED ANOMALY DETECTOR) ===\n",
        "\n",
        "print(\"\\n--- Training Model 2: Autoencoder (Unsupervised) ---\")\n",
        "\n",
        "# --- 4a. Prepare Data for Autoencoder ---\n",
        "# Autoencoders should ONLY be trained on 'normal' data\n",
        "# We get the 'normal' data from the training set\n",
        "X_train_normal = X_train[y_train == 0]\n",
        "print(f\"Autoencoder will be trained on {len(X_train_normal)} 'normal' samples.\")\n",
        "\n",
        "input_dim = X_train.shape[1]\n",
        "encoding_dim = 32  # You can tune this hyperparameter\n",
        "\n",
        "# --- 4b. Define Autoencoder Architecture ---\n",
        "input_layer = Input(shape=(input_dim, ))\n",
        "encoder = Dense(64, activation='relu')(input_layer)\n",
        "encoder = Dense(encoding_dim, activation='relu')(encoder)  # Encoded representation\n",
        "decoder = Dense(64, activation='relu')(encoder)\n",
        "decoder = Dense(input_dim, activation='sigmoid')(decoder) # Output layer\n",
        "\n",
        "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
        "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# --- 4c. Train the Autoencoder ---\n",
        "autoencoder.fit(\n",
        "    X_train_normal,  # Input\n",
        "    X_train_normal,  # Target (it learns to reconstruct itself)\n",
        "    epochs=50,\n",
        "    batch_size=256,\n",
        "    shuffle=True,\n",
        "    validation_split=0.1,  # Use 10% of normal data for validation\n",
        "    verbose=0  # Set to 1 to see training progress\n",
        ")\n",
        "print(\"Autoencoder training complete.\")\n",
        "\n",
        "# --- 4d. Use AE for Anomaly Detection ---\n",
        "# Get the model's reconstructions of the *entire* test set\n",
        "reconstructions = autoencoder.predict(X_test)\n",
        "\n",
        "# Calculate the Mean Squared Error (MSE) for each sample\n",
        "# This is our \"anomaly score\"\n",
        "mse = np.mean(np.power(X_test - reconstructions, 2), axis=1)\n",
        "\n",
        "# --- 4e. Find Anomaly Threshold ---\n",
        "# We need a threshold to decide what error score is an \"anomaly\"\n",
        "# A common method: use the 95th percentile of the 'normal' training data's error\n",
        "reconstructions_normal = autoencoder.predict(X_train_normal)\n",
        "mse_normal = np.mean(np.power(X_train_normal - reconstructions_normal, 2), axis=1)\n",
        "threshold = np.percentile(mse_normal, 95)\n",
        "print(f\"Anomaly threshold (95th percentile of normal error) set to: {threshold:.6f}\")\n",
        "\n",
        "# Classify based on the threshold\n",
        "y_pred_ae = (mse > threshold).astype(int)\n",
        "\n",
        "# === 5. COMPARE PERFORMANCES ===\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"           MODEL PERFORMANCE COMPARISON\")\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "def print_metrics(model_name, y_true, y_pred):\n",
        "    print(f\"--- Metrics for {model_name} ---\")\n",
        "    print(f\"Accuracy:    {accuracy_score(y_true, y_pred):.4f}\")\n",
        "    print(f\"Precision:   {precision_score(y_true, y_pred):.4f}\")\n",
        "    print(f\"Recall:      {recall_score(y_true, y_pred):.4f}\")\n",
        "    print(f\"F1-Score:    {f1_score(y_true, y_pred):.4f}\")\n",
        "    print(f\"AUC-ROC:     {roc_auc_score(y_true, y_pred):.4f}\")\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "    print(\"\\n\")\n",
        "\n",
        "# Print metrics for both models\n",
        "print_metrics(\"XGBoost (Supervised)\", y_test, y_pred_xgb)\n",
        "print_metrics(\"Autoencoder (Unsupervised)\", y_test, y_pred_ae)\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"Script finished.\")"
      ]
    }
  ]
}